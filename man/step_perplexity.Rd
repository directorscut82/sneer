% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tricks.R
\name{step_perplexity}
\alias{step_perplexity}
\title{Step Perplexity}
\usage{
step_perplexity(start_perp, stop_perp, num_iters, num_steps = 10,
  input_weight_fn = exp_weight, verbose = TRUE)
}
\arguments{
\item{start_perp}{Initial target perplexity value for the embedding.}

\item{stop_perp}{Final target perplexity value to be achieved after
\code{num_iters} iterations.}

\item{num_iters}{Number of iterations for the perplexity of the input
probability to change from \code{start_perp} to \code{stop_perp}.}

\item{num_steps}{Number of discrete transitions of the perplexity to take
over \code{num_iters}. Cannot be larger than \code{num_iters}. The larger
this value, the smaller the change in the input probabilities when the
perplexity changes, but the more time spent in calculations.}

\item{input_weight_fn}{Weighting function for distances. It should have the
signature \code{input_weight_fn(d2m, beta)}, where \code{d2m} is a matrix
of squared distances and \code{beta} is a real-valued scalar parameter
which will be varied as part of the search to produce the desired
\code{perplexity}. The function should return a matrix of weights
corresponding to the transformed squared distances passed in as arguments.}

\item{verbose}{If \code{TRUE} print message about tricks during the
embedding.}
}
\value{
tricks callback to vary the perplexity.
}
\description{
A "trick" for improving the quality of the embedding.
}
\details{
Recalculates the input probability at different perplexity values for the
first few iterations of the embedding. Normally, the embedding is begun
at a relatively large perplexity and then the value is reduced to the
usual target value over several iterations, recalculating the input
probabilities. The idea is to avoid poor local minima. Rather than
recalculate the input probabilities at each iteration by a linear decreasing
ramp function, which would be time consuming, the perplexity is reduced
in steps.
}
\examples{
\dontrun{
# Should be passed to the tricks argument of an embedding function.
# Step the perplexity from 150 to 50 over 20 iterations, with 10 steps
# (so two iterations per step)
 embed_prob(tricks = step_perplexity(start_perp = 150, stop_perp = 50,
                                     num_iters = 20, num_steps = 10), ...)
}
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.
}
\seealso{
\code{\link{embed_prob}} for how to use this function for
configuring an embedding. The paper by Venna and co-workers in the references
section describes a very similar method, but with decreasing the bandwidth
of the input weighting function, rather than the perplexity.

Other sneer trick collections: \code{\link{make_tricks}},
  \code{\link{tsne_tricks}}
}

