% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{bold_nag_adapt}
\alias{bold_nag_adapt}
\title{Nesterov Accelerated Gradient Optimizer with Bold Driver and Adaptive Restart}
\usage{
bold_nag_adapt(min_step_size = sqrt(.Machine$double.eps),
  init_step_size = 1, max_momentum = 1, normalize_direction = TRUE,
  linear_weight = TRUE, dec_mult = 0)
}
\arguments{
\item{min_step_size}{Minimum step size allowed.}

\item{init_step_size}{Initial step size.}

\item{max_momentum}{Maximum value the momentum may take.}

\item{normalize_direction}{If \code{TRUE}, scale the length of the direction
to one.}

\item{linear_weight}{If \code{TRUE}, then the contribution of the gradient
descent part of the update is scaled relative to the momentum part.}

\item{dec_mult}{Degree to downweight the momentum iteration number when
restarting.}
}
\value{
Optimizer with NAG parameters and bold driver step size.
}
\description{
Optimizer factory function.
}
\details{
Wrapper around \code{\link{make_opt}} which mixes the NAG descent method and
momentum for non-strongly convex problems formulated by Sutskever et al.,
along with the bold driver method for step size. Additionally the adaptive
restart method of O'Donoghue and Candes.
}
\examples{
# Should be passed to the opt argument of an embedding function:
\dontrun{
 embed_prob(opt = bold_nag_adapt(), ...)
}
}
\seealso{
\code{\link{embed_prob}} and \code{\link{embed_dist}} for how to use
 this function for configuring an embedding.

Other sneer optimization methods: \code{\link{back_nag_adapt}},
  \code{\link{back_nag}}, \code{\link{bold_nag}},
  \code{\link{gradient_descent}}, \code{\link{make_opt}},
  \code{\link{nag}}, \code{\link{optimization_methods}},
  \code{\link{ropt}}, \code{\link{tsne_opt}}
}

