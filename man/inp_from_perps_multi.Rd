% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiscale.R
\name{inp_from_perps_multi}
\alias{inp_from_perps_multi}
\title{Initialize With Multiscale Perplexity}
\usage{
inp_from_perps_multi(perplexities = NULL, input_weight_fn = exp_weight,
  num_scale_iters = NULL, modify_kernel_fn = scale_prec_to_perp,
  verbose = TRUE)
}
\arguments{
\item{perplexities}{List of perplexities to use. If not provided, then
a series of perplexities in decreasing powers of two are used, starting
with the power of two closest to the number of observations in the dataset
divided by four.}

\item{input_weight_fn}{Weighting function for distances. It should have the
signature \code{input_weight_fn(d2m, beta)}, where \code{d2m} is a matrix
of squared distances and \code{beta} is a real-valued scalar parameter
which will be varied as part of the search to produce the desired
perplexity. The function should return a matrix of weights
corresponding to the transformed squared distances passed in as arguments.}

\item{num_scale_iters}{Number of iterations for the perplexity of the input
probability to change from the start perplexity to the end perplexity.}

\item{modify_kernel_fn}{Function to create a new similarity kernel based
on the new perplexity. Will be called every time a new input probability
matrix is generated. See the details section for more.}

\item{verbose}{If \code{TRUE} print message about initialization during the
embedding.}
}
\value{
Input initializer for use by an embedding function.
}
\description{
An initialization method for creating input probabilities.
}
\details{
This function calculates multiple input probability matrices, corresponding
to multiple perplexities, then uses the average of these matrices for the
final probability matrix.

A list of perplexities may be provided to this function. Otherwise, the
perplexities used are decreasing powers of 2, e.g. 16, 8, 4, 2
with the maximum perplexity given by the formula:

\deqn{\lfloor{\log_{2}(N/4)}\rceil}{round(log2(N / 4)}

where N is the number of observations in the data set. The smallest
perplexity value tried is 2.

If a non-zero value of \code{num_scale_iters} is provided, the perplexities
will be combined over the specified number of iterations, averaging over
an increasing number of perplexities until they are all used to generate
the probability matrix at \code{num_scale_iters}. If using the default
scales, the perplexities are added in decreasing order, otherwise, they
are added in the order provided in \code{scales} list. It is suggested that
the \code{scales} list therefore order the perplexities in decreasing order.

The output function used for the embedding also needs to be adapted for each
scale. Because the perplexity of the output can't be directly controlled,
a parameter which can control the 'width' of the similarity function should
be altered. For JSE, which uses an exponential weight function, the value
of the precision parameter, beta, is given by:

\deqn{\beta = K^{-\frac{2U}{P}}}{beta = K ^ (-2U / P)}

where K is the perplexity, P is the output dimensionality (normally 2
for visualization purposes), and U is a coefficient which, when not equal to
1, represents a deviation from the assumption of a uniform density around
each point (e.g. due to clustering, edge effects and so on). Empirically,
Lee and co-workers suggested setting U to a value between 1 and 2, which they
quantified as:

\deqn{U = \textup{min}\left [2, \textup{max} \left( 1, \frac{\hat{D}}{P}\right ) \right ]}
{U = min(2, max(1, D_hat / P))}

and \eqn{\hat{D}}{D_hat} is the intrinsic dimensionality of the dataset.

The intrinsic dimensionality of the dataset is the maximum
intrinsic dimensionality calculated over a set of target perplexities. In
turn, the intrinsic dimensionality for a given perplexity is given by
averaging over the intrinsic dimensionality calculated at each point.
The intrinsic dimensionality for a given data point and perplexity is
calculated by:

\deqn{\hat{D_{i,K}} = -2 \frac{\Delta H_{i,K}}{\Delta \log_{2}\beta_{i,K}}}
{D_hat_ik = -2 * delta_h_ik / delta_log2b_ik}

where \eqn{\frac{\Delta H}{\Delta \log_{2}\beta}}{delta_h_ik / delta_log2b_ik}
is an estimate of the gradient of the Shannon Entropy (in bits) of the input
probability i at perplexity K with respect to the log2 of the input
precision. The estimate is made via a one-sided finite difference
calculation, using the betas and perplexity values from the next largest
perplexity in the \code{perplexities} vector.

Yes, this seems like a lot of work to calculate a value between 1 and 2.

The precision, beta, is then used in the exponential weighting function:

\deqn{W = \exp(-\beta D)}{W = exp(-beta * D2)}

where D2 is the output squared distance matrix and W is the resulting output
weight matrix.

Like the input probability, these output weight matrices are converted to
individual probability matrices and then averaged to create the final
output probability matrix.

If the parameter \code{modify_kernel_fn} is not provided, then the scheme
above is used to create output functions. Any function with a parameter
called \code{beta} can be used, so for example embedding methods which use
the \code{exp_weight} and \code{heavy_tail_weight} weighting
functions can be used with the default function. The signature of
\code{modify_kernel_fn} must be:

\code{modify_kernel_fn(inp, out, method)}

where \code{inp} is the input data, \code{out} is the current output data,
\code{method} is the embedding method.

This function will be called once for each perplexity, and an updated
kernel should be returned.
}
\examples{
\dontrun{
# Should be passed to the init_inp argument of an embedding function.
# Scale the perplexity over 20 iterations, using default perplexities
 embed_prob(init_inp = inp_multiscale(num_scale_iters = 20), ...)
# Scale the perplexity over 20 iterations using the provided perplexities
 embed_prob(init_inp = inp_multiscale(num_scale_iters = 20,
   scales = c(150, 100, 50, 25)), ...)
# Scale using the provided perplexities, but use all of them at once
 embed_prob(init_inp = inp_multiscale(num_scale_iters = 0,
   scales = c(150, 100, 50, 25)), ...)
}
}
\references{
Lee, J. A., Peluffo-Ordo'nez, D. H., & Verleysen, M. (2014).
Multiscale stochastic neighbor embedding: Towards parameter-free
dimensionality reduction. In ESANN.

Lee, J. A., Peluffo-Ordo'nez, D. H., & Verleysen, M. (2015).
Multi-scale similarities in stochastic neighbour embedding: Reducing
dimensionality while preserving both local and global structure.
\emph{Neurocomputing}, \emph{169}, 246-261.
}
\seealso{
\code{embed_prob} for how to use this function for
configuring an embedding.

\code{inp_from_step_perp} also uses multiple
perplexity values, but replaces the old probability matrix with that of the
new perplexity at each step, rather than averaging.

Other sneer input initializers: \code{\link{inp_from_dint_max}},
  \code{\link{inp_from_step_perp}}
}

