% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jse.R
\name{jse_divergence}
\alias{jse_divergence}
\title{Jensen-Shannon Embedding (JSE) Divergence}
\usage{
jse_divergence(pm, qm, zm = NULL, kappa = 0.5, eps = .Machine$double.eps)
}
\arguments{
\item{pm}{Probability Matrix.}

\item{qm}{Probability Matrix.}

\item{zm}{Mixture probability matrix, composed of a weighted sum of
\code{pm} and \code{qm}. If \code{NULL}, will be calculated using the
provided value of \code{kappa}. If provided, the value of \code{kappa} used
to generate should have be the same as the one provided to the function.}

\item{kappa}{Mixture parameter.}

\item{eps}{Small floating point value used to avoid numerical problems.}
}
\value{
JSE divergence between \code{pm} and \code{qm}.
}
\description{
A measure of embedding quality between input and output probability matrices.
}
\details{
The JSE Divergence between two discrete probabilities P and Q
is:

\deqn{D_{JSE}(P||Q)=\frac{1}{1-\kappa}D_{KL}(P||Z) + \frac{1}{\kappa}D_{KL}(Q||Z)}{D_JSE(P||Q) = ((1/(1-kappa))*D_KL(P||Z)) + ((1/kappa)*D_KL(Q||Z))}

where Z is a mixture matrix of \eqn{P} and \eqn{Q}:

\deqn{Z = \kappa P + (1 - \kappa)Q}{Z = kappa * P + (1 - kappa) * Q}

and \eqn{D_{KL}(P||Q)}{D_KL(P||Q)} is the Kullback-Leibler divergence
between \eqn{P} and \eqn{Q}:

\deqn{D_{KL}(P||Q) = \sum_{i}P(i)\log\frac{P(i)}{Q(i)}}{D_KL(P||Q) = sum(Pi*log(Pi/Qi))}

The base of the log determines the units of the divergence.

The JSE divergence is a variation of the Generalized Jensen-Shannon
Divergence for two distributions with the mixing parameter,
\eqn{\kappa}{kappa}, modified so that the divergence has limiting values of
\eqn{D_{KL}(P||Q)}{D_KL(P||Q)} and \eqn{D_{KL}(Q||P)}{D_KL(Q||P)} as
\eqn{\kappa}{kappa} approaches zero and one, respectively.
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.
}

