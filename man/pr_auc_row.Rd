% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/auc.R
\name{pr_auc_row}
\alias{pr_auc_row}
\title{Area Under the PR Curve of an Observation}
\usage{
pr_auc_row(dm, labels, i)
}
\arguments{
\item{dm}{Distance matrix.}

\item{labels}{Vector of labels, of the same size as the number of rows
(or columns) in the distance matrix.}

\item{i}{The row of the distance matrix to use in the PR calculation.}
}
\value{
Area Under the curve.
}
\description{
Embedding quality measure.
}
\details{
The PR curve plots precision (also known as positive predictive value, PPV)
against recall (also known as the true positive rate). The area under the
curve provides similar information compared to the area under the ROC curve,
but may be more appropriate when classes are highly imbalanced.

This function calculates the curve with the label of the specified
observation set as the positive class. The other observations are then
ranked according to their distance from the ith observation
(lower distances being better). Observations with the same label as the
specified observation count as the positive observations.

Perfect retrieval results in an AUC of 1. Random retrieval gives a value
of the proportion of positive class with respect to the entire data set
(e.g. if there are 20 observations with the positive class label in a
dataset of 100, then the random AUC is 0.2).
}
\note{
Use of this function requires that the \code{PRROC} package be
installed.
}
\references{
Keilwagen, J., Grosse, I., & Grau, J. (2014).
Area under precision-recall curves for weighted and unweighted data.
\emph{PloS One}, \emph{9}(3), e92209.

Davis, J., & Goadrich, M. (2006, June).
The relationship between Precision-Recall and ROC curves.
In \emph{Proceedings of the 23rd international conference on Machine
learning}
(pp. 233-240). ACM.
}

