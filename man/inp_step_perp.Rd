% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/initialize_inp.R
\name{inp_step_perp}
\alias{inp_step_perp}
\title{Initialize With Step Perplexity}
\usage{
inp_step_perp(start_perp, stop_perp, num_step_iters, num_steps = 10,
  input_weight_fn = exp_weight, verbose = TRUE)
}
\arguments{
\item{start_perp}{Initial target perplexity value for the embedding.}

\item{stop_perp}{Final target perplexity value to be achieved after
\code{num_iters} iterations.}

\item{num_step_iters}{Number of iterations for the perplexity of the input
probability to change from \code{start_perp} to \code{stop_perp}.}

\item{num_steps}{Number of discrete transitions of the perplexity to take
over \code{num_step_iters}. Cannot be larger than \code{num_step_iters}. The
larger this value, the smaller the change in the input probabilities when the
perplexity changes, but the more time spent in calculations.}

\item{input_weight_fn}{Weighting function for distances. It should have the
signature \code{input_weight_fn(d2m, beta)}, where \code{d2m} is a matrix
of squared distances and \code{beta} is a real-valued scalar parameter
which will be varied as part of the search to produce the desired
\code{perplexity}. The function should return a matrix of weights
corresponding to the transformed squared distances passed in as arguments.}

\item{verbose}{If \code{TRUE} print message about tricks during the
embedding.}
}
\value{
Input initializer for use by an embedding function.
}
\description{
An initialization method for creating input probabilities.
}
\details{
This function initializes the input probabilities with a starting perplexity,
then recalculates the input probability at different perplexity values for the
first few iterations of the embedding. Normally, the embedding is begun
at a relatively large perplexity and then the value is reduced to the
usual target value over several iterations, recalculating the input
probabilities. The idea is to avoid poor local minima. Rather than
recalculate the input probabilities at each iteration by a linear decreasing
ramp function, which would be time consuming, the perplexity is reduced
in steps.
}
\examples{
\dontrun{
# Should be passed to the init_inp argument of an embedding function.
# Step the perplexity from 150 to 50 over 20 iterations, with 10 steps
# (so two iterations per step)
 embed_prob(init_inp = inp_step_perp(start_perp = 150, stop_perp = 50,
                                     num_iters = 20, num_steps = 10), ...)
}
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.
}
\seealso{
\code{\link{embed_prob}} for how to use this function for
configuring an embedding. The paper by Venna and co-workers in the references
section describes a very similar method, but with decreasing the bandwidth
of the input weighting function, rather than the perplexity.

The multiscale approach \code{\link{inp_multiscale}} also uses multiple
perplexity values, but averages the corresponding probability matrices.

Other sneer input initializers: \code{\link{inp_from_perp}},
  \code{\link{inp_multiscale}},
  \code{\link{input_initializers}}
}

