% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jse.R
\name{jse_cost}
\alias{jse_cost}
\title{JSE Cost Function}
\usage{
jse_cost(inp, out, method)
}
\arguments{
\item{inp}{Input data.}

\item{out}{Output data.}

\item{method}{Embedding method.}
}
\value{
JSE divergence between \code{inp$pm} and \code{out$qm}.
}
\description{
A measure of embedding quality between input and output data.
}
\details{
This cost function evaluates the embedding quality by calculating the JSE
divergence, a variation on the generalized Jensen-Shannon divergence between
the input probabilities and the output probabilities. The JSE Divergence
between two discrete probabilities P and Q is:

\deqn{D_{JSE}(P||Q)=\frac{1}{1-\kappa}D_{KL}(P||Z) + \frac{1}{\kappa}D_{KL}(Q||Z)}{D_JSE(P||Q) = ((1/(1-kappa))*D_KL(P||Z)) + ((1/kappa)*D_KL(Q||Z))}

where Z is a mixture matrix of \eqn{P} and \eqn{Q}:

\deqn{Z = \kappa P + (1 - \kappa)Q}{Z = kappa * P + (1 - kappa) * Q}

and \eqn{D_{KL}(P||Q)}{D_KL(P||Q)} is the Kullback-Leibler divergence
between \eqn{P} and \eqn{Q}:

\deqn{D_{KL}(P||Q) = \sum_{i}P(i)\log\frac{P(i)}{Q(i)}}{D_KL(P||Q) = sum(Pi*log(Pi/Qi))}

This cost function requires the following matrices to be defined:
\describe{
 \item{\code{inp$pm}}{Input probabilities.}
 \item{\code{out$qm}}{Output probabilities.}
 \item{\code{out$zm}}{Mixture probabilities: a weighted linear combination
   of \code{inp$pm} and \code{out$qm}.}
}
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.
}
\seealso{
To use \code{out$qm} as the reference probability and calculate the
  divergence of \code{inp$pm} from \code{out$qm}, see
  \code{reverse_kl_cost}.

Other sneer cost functions: \code{\link{nerv_cost}},
  \code{\link{reverse_kl_cost}}
}

