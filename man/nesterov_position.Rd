% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_grad.R
\name{nesterov_position}
\alias{nesterov_position}
\title{Nesterov Accelerated Gradient Calculation Position (Toronto Formulation)}
\usage{
nesterov_position(opt, inp, out, method, iter)
}
\arguments{
\item{opt}{Optimizer.}

\item{inp}{Input data.}

\item{out}{Output data.}

\item{method}{Embedding method.}

\item{iter}{Iteration number.}
}
\value{
New output data.
}
\description{
Function for calculating the position to evaluate the gradient at.
}
\details{
This function return the position of the current solution after the momentum
update for this iteration has been applied. Sustkever and co-workers at
Toronto demonstrated that calculating the gradient at this location was
equivalent to Nesterov Accelerated Gradient (NAG).
}
\references{
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In \emph{Proceedings of the 30th international conference on machine learning (ICML-13)}
(pp. 1139-1147).
}

