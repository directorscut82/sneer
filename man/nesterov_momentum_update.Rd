% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_update.R
\name{nesterov_momentum_update}
\alias{nesterov_momentum_update}
\title{Update Solution With Accelerate Nesterov Momentum}
\usage{
nesterov_momentum_update(opt, inp, out, method, iter)
}
\arguments{
\item{opt}{Optimizer.}

\item{inp}{Input data.}

\item{out}{Output data.}

\item{method}{Embedding method.}

\item{iter}{Iteration number.}
}
\value{
Update matrix, consisting of gradient update and momentum term.
}
\description{
Carries out a solution update using a momentum term in addition to the
gradient update, using the Nesterov acceleration scheme.
}
\details{
This function implements the Nesterov acceleration scheme in the form
presented by Bengio and co-workers. It uses a modified update rule, but
allows gradient to be calculated in the "classical" position, in contrast
to the form given by Sutskever and co-workers, which uses the standard
update rule, but requires the gradient position to be calculated at a
different position.

If this function is used as part of the update method of an optimizer, the
\code{gradient} parameter should be set to \code{\link{classical_gradient}}.
}
\references{
Bengio, Y., Boulanger-Lewandowski, N., & Pascanu, R. (2013, May).
Advances in optimizing recurrent networks.
In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on}
(pp. 8624-8628). IEEE.
}

