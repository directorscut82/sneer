% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_step.R
\name{jacobs}
\alias{jacobs}
\title{Jacobs Step Size Method}
\usage{
jacobs(inc_mult = 1.1, dec_mult = 0.5, inc_fn = partial(`*`, inc_mult),
  dec_fn = partial(`*`, dec_mult), init_step_size = 1,
  min_step_size = .Machine$double.eps, max_step_size = NULL)
}
\arguments{
\item{inc_mult}{Multiplier of the current step size when the cost
decreases. Should be greater than one to increase the step size. This
parameter is ignored if \code{inc_fun} is supplied.}

\item{dec_mult}{Multiplier of the current step size when the cost
increases. Should be smaller than one to decrease the step size. This
parameter is ignored if \code{dec_fun} is supplied.}

\item{inc_fn}{Function to apply to the current step size when the cost
decreases. Should return a value greater than the current step size.}

\item{dec_fn}{Function to apply to the current step size when the cost
increases. Should return a value smaller than the current step size.}

\item{init_step_size}{Step size to attempt on the first step of
optimization.}

\item{min_step_size}{Minimum step size.}

\item{max_step_size}{Maximum step size.}
}
\value{
Jacobs step size method, to be used by the optimizer.
}
\description{
Factory function for creating an optimizer step size method.
}
\details{
This function creates the Jacobs method for step size selection. Also known
as the delta-bar-delta method.

In this implementation, the sign of the gradient is compared to the sign of
the step size at the previous iteration (note that this includes any momentum
term). If the signs are the same, then the step size is increased. If the
signs differ, it is assumed that the minimum has been skipped over, and the
step size is decreased.

There are two differences from the method as originally described by Jacobs:
\enumerate{
 \item As originally described, increases in the step size are achieved by
 adding a fixed amount to current step size, while decreases occur by
 multiplying the step size by a positive value less than one. The default
 arguments here use multipliers for both increase and decrease. See the
 paper by Janet and co-workers mentioned in the references for more details.
 \item In the paper, the sign of the gradient is compared to a weighted
 average of gradients from several previous steps. In this implementation,
 we use only the value from the previous step.
}
}
\examples{
# Use as part of the make_opt function for configuring an optimizer's
# step size method:
make_opt(step_size = jacobs())
}
\references{
R. A. Jacobs.
Increased rates of convergence through learning rate adaptation.
Neural Networks, 1:295-307, 1988.

J. A. Janet, S. M. Scoggins, S. M. Schultz, W. E. Snyder, M. W. White,
J. C. Sutton III
Shocking: an approach to stabilize backprop training with greedy adaptive
learning rates
1998 IEEE International Joint Conference on Neural Networks Proceedings.
IEEE World Congress on Computational Intelligence.
}
\seealso{
The return value of this function is intended for internal use of
the sneer framework only. See \code{\link{optimization_step_size_interface}}
for details on the functions and values defined for this method.
\code{\link{tsne_jacobs}} provides a wrapper around for this method to use
the settings as given in the t-SNE paper.

Other sneer optimization step size methods: \code{\link{bold_driver}},
  \code{\link{constant_step_size}},
  \code{\link{optimization_step_size}},
  \code{\link{tsne_jacobs}}
}

