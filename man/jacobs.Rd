% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_step.R
\name{jacobs}
\alias{jacobs}
\title{Jacobs method step size selection.}
\usage{
jacobs(inc_mult = 1.1, dec_mult = 0.5, inc_fn = partial(`*`, inc_mult),
  dec_fn = partial(`*`, dec_mult), init_step_size = 1,
  min_step_size = .Machine$double.eps, max_step_size = NULL)
}
\arguments{
\item{inc_mult}{Multiplier of the current step size when the cost
decreases. Should be greater than one to increase the step size. This
parameter is ignored if \code{inc_fun} is supplied.}

\item{dec_mult}{Multiplier of the current step size when the cost
increases. Should be smaller than one to decrease the step size. This
parameter is ignored if \code{dec_fun} is supplied.}

\item{inc_fn}{Function to apply to the current step size when the cost
decreases. Should return a value greater than the current step size.}

\item{dec_fn}{Function to apply to the current step size when the cost
increases. Should return a value smaller than the current step size.}

\item{init_step_size}{Step size to attempt on the first step of
optimization.}

\item{min_step_size}{Minimum step size.}

\item{max_step_size}{Maximum step size.}
}
\value{
Step size method, to be used by the Optimizer. A list containing:
 \item{\code{inc_fn}}{Function to invoke to increase the step size.}
 \item{\code{dec_fn}}{Function to invoke to decrease the step size.}
 \item{\code{init_step_size}}{Initial step size.}
 \item{\code{min_step_size}}{Minimum step size.}
 \item{\code{max_step_size}}{Maximum step size.}
 \item{\code{init}}{Function to do any needed initialization.}
 \item{\code{get_step_size}}{Function to return the current step size.}
 \item{\code{validate}}{Function to validate whether the current step was
 successful or not.}
 \item{\code{after_step}}{Function to do any needed updating or internal
 state before the next optimization step.}
}
\description{
This function creates the Jacobs method for step size selection. Also known
as the delta-bar-delta method.
}
\details{
In this implementation, the sign of the gradient is compared to the sign of
the step size at the previous iteration (note that this includes any momentum
term). If the signs are the same, then the step size is increased. If the
signs differ, it is assumed that the minimum has been skipped over, and the
step size is decreased.

There are two differences from the method as originally described by Jacobs:
\enumerate{
 \item As originally described, increases in the step size are achieved by
 adding a fixed amount to current step size, while decreases occur by
 multiplying the step size by a positive value less than one. The default
 arguments here use multipliers for both increase and decrease. See the
 paper by Janet and co-workers mentioned in the references for more details.
 \item In the paper, the sign of the gradient is compared to a weighted
 average of gradients from several previous steps. In this implementation,
 we use only the value from the previous step.
}

To use the settings as given in the t-SNE paper, see the \code{tsne_jacobs}
function.
}
\references{
R. A. Jacobs.
Increased rates of convergence through learning rate adaptation.
Neural Networks, 1:295-307, 1988.

J. A. Janet, S. M. Scoggins, S. M. Schultz, W. E. Snyder, M. W. White,
J. C. Sutton III
Shocking: an approach to stabilize backprop training with greedy adaptive
learning rates
1998 IEEE International Joint Conference on Neural Networks Proceedings.
IEEE World Congress on Computational Intelligence.
}
\seealso{
Other sneer optimization step size methods: \code{\link{bold_driver}},
  \code{\link{tsne_jacobs}}
}

