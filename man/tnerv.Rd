% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nerv.R
\name{tnerv}
\alias{tnerv}
\title{t-Distributed Neighbor Retrieval Visualizer (t-NeRV)}
\usage{
tnerv(eps = .Machine$double.eps, lambda = 0.5, verbose = TRUE)
}
\arguments{
\item{eps}{Small floating point value used to prevent numerical problems,
e.g. in gradients and cost functions.}

\item{lambda}{Weighting factor controlling the emphasis placed on precision
(set \code{lambda} to 0), versus recall (set \code{lambda} to 1). If set to
1, then the method is equivalent to t-SNE. Must be a value between 0 and 1.}

\item{verbose}{If \code{TRUE}, log information about the embedding.}
}
\value{
An embedding method for use by an embedding function.
}
\description{
A probability-based embedding method.
}
\details{
t-NeRV is a variant of t-distributed Stochastic Neighbor Embedding
(\code{\link{tsne}}), with a modified cost function: in addition to
calculating the Kullback-Leibler divergence of the output probabilities Q,
from the input probabilities, P, it also includes the divergence of P from Q.
The final cost function is a weighted sum of these two individual functions.
Hence SSNE is a special case of NeRV where all the weight is placed on the
first component of the cost function.

From an information retrieval perspective, the weighting factor allows the
user to place a relative weight on false positives: points on the embedded
map which have a close distance, but a low input probability, i.e. should not
have been embedded as close neighbors, versus false negatives: pairs with a
large distance in the output coordinates, but a high input probability, i.e.
should have been embedded as close neighbors. From this perspective, t-SNE
is the equivalent of emphasising false positives over false negatives.

The probability matrix used in t-NeRV:

\itemize{
 \item{represents one probability distribution, i.e. the grand sum of the
 matrix is one.}
 \item{is symmetric, i.e. \code{P[i, j] == P[j, i]} and therefore the
 probabilities are joint probabilities.}
}
}
\section{Output Data}{

If used in an embedding, the output data list will contain:
\describe{
 \item{\code{ym}}{Embedded coordinates.}
 \item{\code{qm}}{Joint probability matrix based on the weight matrix
 \code{wm}.}
}
}
\examples{
\dontrun{
# default t-NeRV settings
embed_prob(method = tnerv(lambda = 0.5), ...)

# equivalent to t-SNE
embed_prob(method = tnerv(lambda = 1), ...)

# puts an emphasis on precision over recall and allows long tails
# will create widely-separated small clusters
embed_prob(method = tnerv(lambda = 0), ...)
}
}
\references{
Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.
}
\seealso{
NeRV uses the \code{\link{nerv_cost}} cost function and the
  \code{\link{tdist_weight}} similarity function for converting distances to
  probabilities.
The return value of this function should be used with the
 \code{\link{embed_prob}} embedding function.

Other sneer embedding methods: \code{\link{asne}},
  \code{\link{embedding_methods}}, \code{\link{hsjse}},
  \code{\link{hsnerv}}, \code{\link{hssne}},
  \code{\link{jse}}, \code{\link{mmds}},
  \code{\link{nerv}}, \code{\link{rasne}},
  \code{\link{rssne}}, \code{\link{rtsne}},
  \code{\link{sammon_map}}, \code{\link{smmds}},
  \code{\link{snerv}}, \code{\link{ssne}},
  \code{\link{tasne}}, \code{\link{tsne}}

Other sneer probability embedding methods: \code{\link{asne}},
  \code{\link{hsjse}}, \code{\link{hsnerv}},
  \code{\link{hssne}}, \code{\link{jse}},
  \code{\link{nerv}},
  \code{\link{probability_embedding_methods}},
  \code{\link{rasne}}, \code{\link{rssne}},
  \code{\link{rtsne}}, \code{\link{snerv}},
  \code{\link{ssne}}, \code{\link{tasne}},
  \code{\link{tsne}}
}

