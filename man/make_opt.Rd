% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{make_opt}
\alias{make_opt}
\title{Optimizer}
\usage{
make_opt(gradient = classical_gradient(), direction = steepest_descent(),
  step_size = bold_driver(), update = no_momentum(),
  normalize_grads = TRUE, recenter = TRUE, mat_name = "ym")
}
\arguments{
\item{gradient}{Method to calculate the gradient at a solution
position. Set by calling one of the configuration functions listed in
\code{\link{optimization_gradient}}.}

\item{direction}{Method to calculate the direction to move. Set
by calling one of the configuration functions listed in
\code{\link{optimization_direction}}.}

\item{step_size}{Method to calculate the step size of the direction. Set
by calling one of the configuration functions listed in
\code{\link{optimization_step_size}}.}

\item{update}{Method to combine a gradient descent with other terms (e.g.
momentum) to produce the final update. Set by calling one of the
configuration functions listed in \code{\link{optimization_update}}.}

\item{normalize_grads}{If \code{TRUE} the gradient matrix is normalized to
a length of one before step size calculation.}

\item{recenter}{If \code{TRUE}, recenter the coordinates after each
optimization step.}

\item{mat_name}{Name of the matrix in the output list \code{out} which
contains the embedded coordinates.}
}
\value{
Optimizer.
}
\description{
Function to create an optimization method, used in an embedding function.
}
\details{
The optimizer consists of
\itemize{
 \item A method to calculate the gradient at a certain position.
 \item A method to calculate the direction to move in from the gradient
 position.
 \item A method to calculate the size of the step to move.
 \item A method to calculate the update of the solution, consisting of the
 gradient descent (as calculated by the previous three functions) and any
 extra modifiers, normally a momentum term based on the previous solution
 step.
}

Normalizing the gradients (by setting \code{normalize_grads} to
\code{TRUE}) will scale the gradient matrix to length 1 before the
optimization step. This has the effect of making the size of the gradient
descent dependent only on the step size, rather than the product of the
step size and the size of the gradient. This can increase the stability of
step size methods like Bold Driver and the Jacobs method which iteratively
update the step size based on previous values rather than doing a search,
or with methods where the gradients can get extremely large (e.g. in
traditional distance-based embeddings like MDS and Sammon mapping which
involve dividing by potentially very small distances).

The optimizer can validate the proposed solution, rejecting it
if the solution is not acceptable of the methods it is comprised of. It also
control updating the internal state of the methods (e.g. step size and
momentum).
}
\examples{
# Steepest descent with Jacobs adaptive step size and step momentum, as
# used in the t-SNE paper.
make_opt(step_size = jacobs(inc_fn = partial(`+`, 0.2), dec_mult = 0.8,
                            min_step_size = 0.1),
         update = step_momentum(init_momentum = 0.5, final_momentum = 0.8,
                                switch_iter = 250),
         normalize_grads = FALSE)

# Use bold driver adaptive step size (1\% step size increase, 25\% decrease)
# with step momentum and normalizing gradients.
make_opt(step_size = bold_driver(inc_mult = 1.01, dec_mult = 0.75),
         update = step_momentum(init_momentum = 0.5, final_momentum = 0.8,
                                switch_iter = 250),
         normalize_grads = TRUE)

# Nesterov Accelerated Gradient optimizer with bold driver adaptive step size
make_opt(gradient = nesterov_gradient(), step_size = bold_driver(),
         update = nesterov_nsc_momentum())

# Should be passed to the opt argument of an embedding function:
\dontrun{
 embed_prob(opt = make_opt(gradient = nesterov_gradient(),
                          step_size = bold_driver(),
                          update = nesterov_nsc_momentum()), ...)
}
}
\seealso{
\code{\link{embed_prob}} for how to use this function for configuring
  an embedding.

Other sneer optimization methods: \code{\link{bold_nag}},
  \code{\link{gradient_descent}},
  \code{\link{nag_montreal}}, \code{\link{nag_toronto}},
  \code{\link{optimization_methods}},
  \code{\link{tsne_opt}}
}

