% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{make_opt}
\alias{make_opt}
\title{Create optimizer.}
\usage{
make_opt(gradient = classical_gradient(), direction = steepest_descent(),
  step_size = bold_driver(), update = no_momentum(),
  normalize_grads = TRUE, recenter = TRUE, mat_name = "ym")
}
\arguments{
\item{gradient}{Method to calculate the gradient at a solution
position. Set by calling a configuration function:
\itemize{
 \item \code{classical_gradient()} Calculates the gradient at the position of
 the current solution. This is the default setting.
 \item \code{nesterov_gradient()} Uses the Nesterov Accelerated Gradient
 method. A suitable \code{update} scheme should be used if this option is
 chosen, e.g. \code{nesterov_nsc_momentum}.
}}

\item{direction}{Method to calculate the direction to move. Set by calling a
configuration function:
\itemize{
 \item \code{steepest_descent()} Move in the direction of steepest descent.
 This is the default setting and it's the only currently defined method,
 so that's easy.
}}

\item{step_size}{Method to calculate the step size of the direction. Set
by calling a configuration function:
\itemize{
 \item \code{jacobs()} Jacobs method. Used in the t-SNE paper.
 \item \code{bold_driver()} Bold driver.
}}

\item{update}{Method to combine a gradient descent with other terms (e.g.
momentum) to produce the final update. Set by calling a configuration
function:
\itemize{
 \item \code{step_momentum()} Step momentum schedule. Used in the t-SNE paper.
 \item \code{linear_momentum()} Linear momentum schedule.
 \item \code{nesterov_nsc_momentum()} Nesterov momentum for non-strongly
 convex problems. Use when the \code{gradient} method is
 \code{nesterov_gradient}.
 \item \code{no_momentum()} Don't use a momentum term, optimization will
 only use gradient descent to update the solution. The default setting.
}}

\item{normalize_grads}{If \code{TRUE} the gradient matrix is normalized to
a length of one.}

\item{recenter}{If \code{TRUE}, recenter the coordinates after each
optimization step.}

\item{mat_name}{Name of the matrix in the output list \code{out} which
contains the embedded coordinates.}
}
\value{
Optimizer.
}
\description{
Function to create an optimization method. The optimizer consists of
\itemize{
 \item A method to calculate the gradient at a certain position.
 \item A method to calculate the direction to move in from the gradient
 position.
 \item A method to calculate the size of the step to move.
 \item A method to calculate the update of the solution, consisting of the
 gradient descent (as calculated by the previous three functions) and any
 extra modifiers, normally a momentum term based on the previous solution
 step.
}
}
\details{
Normalizing the gradients (by setting \code{normalize_grads} to
\code{TRUE}) will scale the gradient matrix to length 1 before the
optimization step. This has the effect of making the size of the gradient
descent dependent only on the step size, rather than the product of the
step size and the size of the gradient. This can increase the stability of
step size methods like Bold Driver and the Jacobs method which iteratively
update the step size based on previous values rather than doing a search,
or with methods where the gradients can get extremely large (e.g. in
traditional distance-based embeddings like MDS and Sammon mapping which
involve dividing by potentially very small distances).

The optimizer can validate the proposed solution, rejecting it
if the solution is not acceptable of the methods it is comprised of. It also
control updating the internal state of the methods (e.g. step size and
momentum).
}
\examples{
# Steepest descent with Jacobs adaptive step size and step momentum, as
# used in the t-SNE paper.
make_opt(step_size = jacobs(inc_fn = partial(`+`, 0.2), dec_mult = 0.8,
                            min_step_size = 0.1),
         update = step_momentum(init_momentum = 0.5, final_momentum = 0.8,
                                switch_iter = 250),
         normalize_grads = FALSE)

# Use bold driver adaptive step size (1\% step size increase, 25\% decrease)
# with step momentum and normalizing gradients.
make_opt(step_size = bold_driver(inc_mult = 1.01, dec_mult = 0.75),
         update = step_momentum(init_momentum = 0.5, final_momentum = 0.8,
                                switch_iter = 250),
         normalize_grads = TRUE)

# Nesterov Accelerated Gradient optimizer with bold driver adaptive step size
make_opt(gradient = nesterov_gradient(), step_size = bold_driver(),
         update = nesterov_nsc_momentum())

# Should be passed to the opt argument of an embedding function:
\dontrun{
 embed_sim(opt = make_opt(gradient = nesterov_gradient(),
                          step_size = bold_driver(),
                          update = nesterov_nsc_momentum()), ...)
}
}
\seealso{
\code{\link{embed_sim}} for how to use this function for configuring
an embedding, and \code{\link{tsne_opt}} and \code{\link{bold_nag_opt}} for
some convenience functions that choose a set of defaults for you.
}

