% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim_external.R
\name{ropt}
\alias{ropt}
\title{R Optimizer}
\usage{
ropt(method = "CG", batch_iter = 20, ...)
}
\arguments{
\item{method}{The optimization method to be used.}

\item{batch_iter}{Number of steps of minimization to carry out per invocation
of the optimizer.}

\item{...}{Other options to pass to the \code{control} parameter of the
underlying \code{\link[stats]{optim}} function.}
}
\value{
Optimizer using R general-purpose optimization methods.
}
\description{
Function to create an optimizer, using the R general-purpose optimization
methods.
}
\details{
This uses the optimization methods in \code{\link[stats]{optim}}, notably
the conjugate gradient method (used in the NeRV paper), and the low-memory
approximation to BFGS (used in the multiscale JSE paper).

Their only downside is that you have much less control over their inner
workings, and they run synchronously until the specified number of
iterations, given by the \code{batch_iter} parameter, has been run
(or convergence causes the routine to exit early). During those iterations,
the rest of the embedding parameters (e.g. tricks and reports) won't get
a chance to run.

However, each call to the R optimizer is only treated as one step of
optimization from the point of view of the embedding algorithm. As a result,
you should be aware that if you swap a home-grown optimizer
created by \code{make_opt} with one using \code{ropt} and
\code{batch_iter = 10}, if you don't change any of the other parameters,
(most pertinently, the value of \code{max_iter}), your optimizer may run for
much longer (maybe ten times longer) than you were expecting.

This might be a bit annoying, but it's probably better than the alternative
which might seem more natural: increment the number of embedding iterations
by the size of \code{batch_iter} after each call of the R optimizer. The
problem with this approach is that from the point of view of all the other
components of the embedding algorithm, the iteration number jumped from e.g.
10 to 20, without any value in between. This would be bad news if you had
set up a "trick" to reduce the input probability perplexity of an
embedding by reducing it at iterations 12, 14 and 16. The trick part of the
embedding never sees the iteration at these values, so they never run.
Similarly, if you set the report to run every 25 steps, you would fail
to run the report half the time.

DO NOT just set \code{batch_iter} to 1 to try and get round this problem.
The R optimizer only retains the memory of the current optimization within
each batch. As both CG and L-BFGS methods rely on building up a "memory" of
previous descent directions, setting the \code{batch_iter} to too low a
value will reduce their behavior to steepest descent. A \code{batch_iter} of
15-25 seems adequate.
}
\examples{
# Conjugate Gradient with the Polak-Ribiere method. 20 steps of optimization.
ropt(method = "CG", type = 2, batch_iter = 20)

# low-memory version of BFGS. 15 steps of optimization per invocation.
ropt(method = "L-BFGS-B", batch_iter = 15)

\dontrun{
# Should be passed to the opt argument of an embedding function.
# Total number of iterations is 100, so the R optimizer will be invoked
# 10 times, each time running up to 10 iterations. The reporter will
# report after every 20 iterations of the embedding function.
 embed_prob(opt = ropt(method = "L-BFGS-B", batch_iter = 10),
            reporter = make_reporter(..., report_every = 20),
            max_iter = 100)

}
}
\seealso{
\code{\link[stats]{optim}} for more details on what options
 can be passed.

Other sneer optimization methods: \code{\link{back_nag_adapt}},
  \code{\link{back_nag}}, \code{\link{bold_nag_adapt}},
  \code{\link{bold_nag}}, \code{\link{gradient_descent}},
  \code{\link{make_opt}}, \code{\link{nag}},
  \code{\link{optimization_methods}},
  \code{\link{tsne_opt}}
}

