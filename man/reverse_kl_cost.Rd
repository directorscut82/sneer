% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nerv.R
\name{reverse_kl_cost}
\alias{reverse_kl_cost}
\title{Reverse Kullback-Leibler Divergence Cost Function}
\usage{
reverse_kl_cost(inp, out, method)
}
\arguments{
\item{inp}{Input data.}

\item{out}{Output data.}

\item{method}{Embedding method.}
}
\value{
KL divergence between \code{inp$pm} and \code{out$qm}.
}
\description{
A measure of embedding quality between input and output data.
}
\details{
This cost function the embedding quality by calculating the KL divergence
between the input probabilities and the output probabilities, where the
output probabilities are considered the reference probabilities.

This cost function requires the following matrices to be defined:
\describe{
 \item{\code{inp$pm}}{Input probabilities.}
 \item{\code{out$qm}}{Output probabilities.}
}
}
\seealso{
\code{\link{kl_cost}} provides more detail on the differences
  between the usual KL divergence and this "reverse" divergence.

Other sneer cost functions: \code{\link{kl_cost}},
  \code{\link{kruskal_stress_cost}},
  \code{\link{mean_relative_error_cost}},
  \code{\link{metric_sstress_cost}},
  \code{\link{metric_stress_cost}},
  \code{\link{nerv_cost}},
  \code{\link{normalized_stress_cost}},
  \code{\link{rms_metric_stress_cost}},
  \code{\link{sammon_stress_cost}}
}

