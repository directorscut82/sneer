% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{nag_toronto}
\alias{nag_toronto}
\title{Nesterov Accelerated Gradient (Toronto Formulation)}
\usage{
nag_toronto(opt)
}
\arguments{
\item{opt}{Optimizer.}
}
\value{
Optimizer implementing the NAG.
}
\description{
Wrapper to apply NAG to an optimizer.
}
\details{
Given a classical gradient descent optimizer with a momentum term, return
the optimizer implementing the Nesterov Accelerated Gradient method. This
uses the formulation suggested by the Hinton group at Toronto, which involves
evaluating the gradient at the position after the momentum update.
}
\examples{
# boring old optimizer
opt <- make_opt(step_size = constant_step_size(0.1),
                update = constant_momentum(0.8))

# nesterovize it in the Toronto style
opt <- nag_toronto(opt)
}
\references{
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In \emph{Proceedings of the 30th international conference on machine learning (ICML-13)}
(pp. 1139-1147).

Sutskever, I. (2013).
\emph{Training recurrent neural networks}
(Doctoral dissertation, University of Toronto).
}
\seealso{
\code{\link{nag_montreal}} is an alternative formulation of NAG
suggested by the Bengio group at Montreal.
}

