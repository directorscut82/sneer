% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{bold_nag}
\alias{bold_nag}
\title{Nesterov Accelerated Gradient Optimizer with Bold Driver}
\usage{
bold_nag(min_step_size = sqrt(.Machine$double.eps), init_step_size = 1,
  max_momentum = 1, normalize_direction = TRUE, linear_weight = TRUE)
}
\arguments{
\item{min_step_size}{Minimum step size allowed.}

\item{init_step_size}{Initial step size.}

\item{max_momentum}{Maximum value the momentum may take.}

\item{normalize_direction}{If \code{TRUE}, scale the length of the gradient to
one.}

\item{linear_weight}{If \code{TRUE}, then the contribution of the gradient
descent part of the update is scaled relative to the momentum part.}
}
\value{
Optimizer with NAG parameters and bold driver step size.
}
\description{
Optimizer factory function.
}
\details{
Wrapper around \code{make_opt} which mixes the NAG descent method and
momentum for non-strongly convex problems formulated by Sutskever et al.,
along with the bold driver method for step size.
}
\note{
This optimizer is prone to converge prematurely in the face of sudden
changes to the solution landscape, such as can happen when certain
\code{tricks} are applied. In these cases, substantially increasing
the \code{min_step_size} parameter so that the bold driver doesn't reduce
the step size is highly recommended.
}
\examples{
# Should be passed to the opt argument of an embedding function:
\dontrun{
 embed_prob(opt = bold_nag(), ...)
}
}
\seealso{
\code{embed_prob} and \code{embed_dist} for how to use
 this function for configuring an embedding.

Other sneer optimization methods: \code{\link{back_nag_adapt}},
  \code{\link{back_nag}}, \code{\link{bold_nag_adapt}},
  \code{\link{gradient_descent}}, \code{\link{make_opt}},
  \code{\link{nag}}, \code{\link{optimization_methods}},
  \code{\link{ropt}}, \code{\link{tsne_opt}}
}

