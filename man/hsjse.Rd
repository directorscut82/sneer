% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/jse.R
\name{hsjse}
\alias{hsjse}
\title{Heavy-Tailed Symmetric Jensen-Shannon Embedding (HSJSE)}
\usage{
hsjse(kappa = 0.5, alpha = 0, beta = 1, eps = .Machine$double.eps,
  verbose = TRUE)
}
\arguments{
\item{kappa}{Mixture parameter. Cost function behaves more like the
Kullback-Leibler divergence as it approaches zero and more like the
"reverse" KL divergence as it approaches one.}

\item{alpha}{Tail heaviness. Must be greater than zero. Set to zero for
a Gaussian-like kernel, and to one for a Student-t distribution.}

\item{beta}{The precision of the function. Becomes equivalent to the
precision in the Gaussian distribution of distances as \code{alpha}
approaches zero.}

\item{eps}{Small floating point value used to prevent numerical problems,
e.g. in gradients and cost functions.}

\item{verbose}{If \code{TRUE}, log information about the embedding.}
}
\value{
An embedding method for use by an embedding function.
}
\description{
A probability-based embedding method.
}
\details{
HSJSE is a variant of \code{\link{jse}} which uses a symmetrized, normalized
probability distribution like \code{\link{ssne}}, rather than the that used
by the original JSE method, which used the unnormalized distributions of
\code{\link{asne}}.

Additionally, it uses the heavy-tailed kernel function of
\code{\link{hssne}}, to generalize exponential and t-distributed weighting.
By modifying the \code{alpha} and \code{kappa} parameters, this embedding
method can reproduce multiple embedding methods (see the examples section).

The probability matrix used in HSJSE:

\itemize{
 \item{represents one probability distribution, i.e. the grand sum of the
 matrix is one.}
 \item{is symmetric, i.e. \code{P[i, j] == P[j, i]} and therefore the
 probabilities are joint probabilities.}
}
}
\section{Output Data}{

If used in an embedding, the output data list will contain:
\describe{
 \item{\code{ym}}{Embedded coordinates.}
 \item{\code{qm}}{Joint probability matrix based on the weight matrix
 \code{wm}.}
}
}
\examples{
\dontrun{
# default HSJSE, cost function is symmetric
embed_prob(method = hsjse(kappa = 0.5), ...)

# equivalent to SSNE
embed_prob(method = hsjse(kappa = 0, alpha = 0), ...)

# equivalent to "reverse" SSNE
embed_prob(method = hsjse(kappa = 1, alpha = 0), ...)

# equivalent to t-SNE
embed_prob(method = hsjse(kappa = 0, alpha = 1), ...)

# equivalent to "reverse" t-SNE
embed_prob(method = hsjse(kappa = 1, alpha = 1), ...)
}
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Yang, Z., King, I., Xu, Z., & Oja, E. (2009).
Heavy-tailed symmetric stochastic neighbor embedding.
In \emph{Advances in neural information processing systems} (pp. 2169-2177).
}
\seealso{
HSJSE uses the \code{\link{jse_cost}} cost function and the
  \code{\link{heavy_tail_weight}} similarity function for converting
  distances to probabilities. The \code{\link{hsnerv}} embedding method is
  similar.
The return value of this function should be used with the
\code{\link{embed_prob}} embedding function.

Other sneer embedding methods: \code{\link{asne}},
  \code{\link{embedding_methods}}, \code{\link{hsnerv}},
  \code{\link{hssne}}, \code{\link{jse}},
  \code{\link{mmds}}, \code{\link{nerv}},
  \code{\link{rasne}}, \code{\link{rssne}},
  \code{\link{rtsne}}, \code{\link{sammon_map}},
  \code{\link{smmds}}, \code{\link{snerv}},
  \code{\link{ssne}}, \code{\link{tasne}},
  \code{\link{tnerv}}, \code{\link{tsne}}

Other sneer probability embedding methods: \code{\link{asne}},
  \code{\link{hsnerv}}, \code{\link{hssne}},
  \code{\link{jse}}, \code{\link{nerv}},
  \code{\link{probability_embedding_methods}},
  \code{\link{rasne}}, \code{\link{rssne}},
  \code{\link{rtsne}}, \code{\link{snerv}},
  \code{\link{ssne}}, \code{\link{tasne}},
  \code{\link{tnerv}}, \code{\link{tsne}}
}

