% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cost.R
\name{kl_divergence_rows}
\alias{kl_divergence_rows}
\title{Kullback-Leibler Divergence per Row}
\usage{
kl_divergence_rows(pm, qm, eps = .Machine$double.eps)
}
\arguments{
\item{pm}{Row probability Matrix. First probability in the divergence.}

\item{qm}{Row Probability Matrix. Second probability in the divergence.}

\item{eps}{Small floating point value used to avoid numerical problems.}
}
\value{
Vector of KL divergences from \code{qm} to \code{pm}.
}
\description{
A measure of embedding quality between input and output row probability
matrices.
}
\details{
The Kullback-Leibler Divergence between two discrete probabilities P and Q
is:

\deqn{D_{KL}(P||Q) = \sum_{i}P(i)\log\frac{P(i)}{Q(i)}}{D_KL(P||Q) = sum(Pi*log(Pi/Qi))}

The base of the log determines the units of the divergence.

This function calculates the KL for each distribution in the provided
matrices, one per row.
}

