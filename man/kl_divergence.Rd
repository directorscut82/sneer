% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cost.R
\name{kl_divergence}
\alias{kl_divergence}
\title{Kullback-Leibler Divergence}
\usage{
kl_divergence(pm, qm, eps = .Machine$double.eps)
}
\arguments{
\item{pm}{Probability Matrix. First probability in the divergence.}

\item{qm}{Probability Matrix. Second probability in the divergence.}

\item{eps}{Small floating point value used to avoid numerical problems.}
}
\value{
KL divergence between \code{pm} and \code{qm}.
}
\description{
A measure of embedding quality between input and output probability matrices.
}
\details{
The Kullback-Leibler Divergence between two discrete probabilities P and Q
is:

\deqn{D_{KL}(P||Q) = \sum_{i}P(i)\log\frac{P(i)}{Q(i)}}{D_KL(P||Q) = sum(Pi*log(Pi/Qi))}

The base of the log determines the units of the divergence.

If a row probability matrix is provided (where each row in the matrix is
a separate distribution), this function returns the sum of all divergences.
}
\seealso{
\code{\link{kl_divergence_rows}} to obtain the separate KL
divergences when using row probability matrices.
}

