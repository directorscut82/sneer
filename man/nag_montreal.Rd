% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{nag_montreal}
\alias{nag_montreal}
\title{Nesterov Accelerated Gradient (Montreal Formulation)}
\usage{
nag_montreal(opt)
}
\arguments{
\item{opt}{Optimizer.}
}
\value{
Optimizer implementing the NAG.
}
\description{
Wrapper to apply NAG to an optimizer.
}
\details{
Given a classical gradient descent optimizer with a momentum term, return
the optimizer implementing the Nesterov Accelerated Gradient method. This
uses the formulation suggested by the Bengio group at Montreal, which
involves a modified momentum term.
}
\examples{
# boring old optimizer
opt <- make_opt(step_size = constant_step_size(0.1),
                update = constant_momentum(0.8))

# nesterovize it in the Montreal style
opt <- nag_montreal(opt)
}
\references{
Bengio, Y., Boulanger-Lewandowski, N., & Pascanu, R. (2013, May).
Advances in optimizing recurrent networks.
In \emph{Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on}
(pp. 8624-8628). IEEE.
}

