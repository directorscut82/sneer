% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optim.R
\name{nag}
\alias{nag}
\title{Nesterov Accelerated Gradient (Toronto Formulation)}
\usage{
nag(...)
}
\arguments{
\item{...}{step size and update parameters for creating an optimizer.}
}
\value{
Optimizer implementing the NAG method.
}
\description{
Wrapper to create a NAG optimizer.
}
\details{
Create an optimizer implementing the Nesterov Accelerated Gradient method.
This uses the formulation suggested by the Hinton group at Toronto, which
involves evaluating the gradient at the position after the momentum update.
}
\examples{
# boring old optimizer
opt <- make_opt(step_size = constant_step_size(0.1),
                update = constant_momentum(0.8))

# nesterov accelerated version
opt <- nag(step_size = constant_step_size(0.1),
           update = constant_momentum(0.8))
}
\references{
Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In \emph{Proceedings of the 30th international conference on machine learning (ICML-13)}
(pp. 1139-1147).

Sutskever, I. (2013).
\emph{Training recurrent neural networks}
(Doctoral dissertation, University of Toronto).
}
\seealso{
Other sneer optimization methods: \code{\link{bold_nag}},
  \code{\link{gradient_descent}}, \code{\link{make_opt}},
  \code{\link{optimization_methods}}, \code{\link{ropt}},
  \code{\link{tsne_opt}}
}

