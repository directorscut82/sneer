% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/auc.R
\name{pr_auc}
\alias{pr_auc}
\title{Average Area Under the Precision-Recall Curve}
\usage{
pr_auc(inp, out)
}
\arguments{
\item{inp}{Input data. This should be storing the class labels as
\code{inp$labels}, as vector with the labels ordered in the same way as
the observations in the distance matrices.}

\item{out}{Output data. If the output distance matrix is not stored as
\code{out$dm}, it will be calculated.}
}
\value{
Area Under the Precision-Recall curve, averaged over each
observation.
}
\description{
Embedding quality measure.
}
\details{
The PR curve plots precision (also known as positive predictive value, PPV)
against recall (also known as the true positive rate). The area under the
curve provides similar information compared to the area under the ROC curve,
but may be more appropriate when classes are highly imbalanced.

This function calculates the PR curve N times, where N is the number of the
observations. The label of the Nth observation is set as the positive class
and then the other observations are ranked according to their distance from
the Nth observation in the output coordinates (lower distances being better).
Observations with the same label as the Nth observation count as positive
observations. The final reported result is the average over all observations.

Perfect retrieval results in an AUC of 1. For random retrieval, the value
is the proportion of the positive class labels for that curve.
}
\note{
Use of this function requires that the \code{PRROC} package be
installed.
}
\references{
Keilwagen, J., Grosse, I., & Grau, J. (2014).
Area under precision-recall curves for weighted and unweighted data.
\emph{PloS One}, \emph{9}(3), e92209.

Davis, J., & Goadrich, M. (2006, June).
The relationship between Precision-Recall and ROC curves.
In \emph{Proceedings of the 23rd international conference on Machine
learning}
(pp. 233-240). ACM.
}

