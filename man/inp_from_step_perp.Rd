% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/multiscale.R
\name{inp_from_step_perp}
\alias{inp_from_step_perp}
\title{Initialize With Step Perplexity}
\usage{
inp_from_step_perp(perplexities = NULL, input_weight_fn = exp_weight,
  num_scale_iters = 20, modify_kernel_fn = scale_prec_to_perp,
  verbose = TRUE)
}
\arguments{
\item{perplexities}{List of perplexities to use. If not provided, then
ten equally spaced perplexities will be used, starting at half the size
of the dataset, and ending at 32.}

\item{input_weight_fn}{Weighting function for distances. It should have the
signature \code{input_weight_fn(d2m, beta)}, where \code{d2m} is a matrix
of squared distances and \code{beta} is a real-valued scalar parameter
which will be varied as part of the search to produce the desired
perplexity. The function should return a matrix of weights
corresponding to the transformed squared distances passed in as arguments.}

\item{num_scale_iters}{Number of iterations for the perplexity of the input
probability to change from the start perplexity to the end perplexity.}

\item{modify_kernel_fn}{Function to create a new similarity kernel based
on the new perplexity. Will be called every time a new input probability
matrix is generated. See the details section for more.}

\item{verbose}{If \code{TRUE} print message about tricks during the
embedding.}
}
\value{
Input initializer for use by an embedding function.
}
\description{
An initialization method for creating input probabilities.
}
\details{
This function initializes the input probabilities with a starting perplexity,
then recalculates the input probability at different perplexity values for
the first few iterations of the embedding. Normally, the embedding is begun
at a relatively large perplexity and then the value is reduced to the
usual target value over several iterations, recalculating the input
probabilities. The idea is to avoid poor local minima. Rather than
recalculate the input probabilities at each iteration by a linear decreasing
ramp function, which would be time consuming, the perplexity is reduced
in steps.

You will need to decide what to do about the output function: should its
parameters change as the input probabilities change? You could decide to
do nothing, especially if you're using a kernel without any parameters, such
as the Student t-distribution used in t-SNE, but you will need to explicitly
set the \code{modify_kernel_fn} parameter to \code{NULL}.

By default, the kernel function will try the  suggestion of Lee and
co-workers, which is to scale the beta parameter of the exponential kernel
function used in many embedding methods so that as the perplexity gets
smaller, the beta value gets larger, thus reducing the bandwidth of the
kernel. See the \code{\link{scale_prec_to_perp}} function for more details.
If your kernel function doesn't have a \code{beta} parameter, the function
will still run but have no effect on the output kernel.
}
\examples{
\dontrun{
# Should be passed to the init_inp argument of an embedding function.
# Step the perplexity from 75 to 25 with 6 values inclusive, taking 20
# iterations overall (so 4 iterations per step)
 embed_prob(init_inp = inp_from_step_perp(
   perplexities = seq(75, 25, length.out = 6), num_scale_iters = 20), ...)
}
}
\references{
Lee, J. A., Renard, E., Bernard, G., Dupont, P., & Verleysen, M. (2013).
Type 1 and 2 mixtures of Kullback-Leibler divergences as cost functions in
dimensionality reduction based on similarity preservation.
\emph{Neurocomputing}, \emph{112}, 92-108.

Venna, J., Peltonen, J., Nybo, K., Aidos, H., & Kaski, S. (2010).
Information retrieval perspective to nonlinear dimensionality reduction for
data visualization.
\emph{Journal of Machine Learning Research}, \emph{11}, 451-490.

The paper by Venna and co-workers describes a very similar approach, but
with decreasing the bandwidth of the input weighting function, rather than
the perplexity. This is because NeRV sets the bandwidths of the output
exponential similarity kernel to those from the input kernel.
}
\seealso{
\code{\link{embed_prob}} for how to use this function for
configuring an embedding.

Other sneer input initializers: \code{\link{inp_from_perps_multi}},
  \code{\link{inp_from_perp}},
  \code{\link{input_initializers}}
}

